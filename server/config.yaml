# ccoli server configuration

server:
  host: "0.0.0.0"
  port: 5001

stt:
  model_size: "medium"      # tiny, base, small, medium, large
  device: "cuda"            # cuda or cpu
  language: "ko"

llm:
  base_url: "http://localhost:11434"
  model: "qwen3:8b"
  think: false
  auto_start: true
  start_command: "ollama serve"
  startup_timeout: 10.0

tts:
  voice: "ko-KR-SunHiNeural"

assistant:
  name: "ccoli"
  personality: "witty"
  proactive: true
  proactive_interval: 600

features:
  robot_mode_enabled: false

memory:
  refresh_interval: 5
  memory_dir: "memory"

weather:
  api_key: ""
  lat: 37.5665
  lon: 126.9780

context:
  max_history: 20
  backup_interval: 10
  auto_save: true

emotion:
  enabled: true
  decay_to_neutral: true
  decay_interval: 300

logging:
  level: "INFO"
  save_to_file: true
  log_dir: "logs"

connection:
  socket_timeout: 0.5

queue:
  stt_maxsize: 4
  tts_maxsize: 2
  command_maxsize: 10

audio:
  max_seconds: 12
