"""
AgentMode â€” ì½œë¦¬ (Colly) í™ˆ ì—ì´ì „íŠ¸
MemoryManager ê¸°ë°˜ êµ¬ì¡°í™”ëœ ë©”ëª¨ë¦¬ + ìë¹„ìŠ¤ ìŠ¤íƒ€ì¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
"""

import json
import asyncio
import logging
import numpy as np
from datetime import datetime
from typing import Optional

from emotion_system import EmotionSystem
from info_services import InfoServices
from proactive_interaction import ProactiveInteraction
from scheduler import Scheduler
from memory_manager import MemoryManager

log = logging.getLogger("agent_mode")


class AgentMode:
    def __init__(self, device="cuda", weather_api_key=None, location="Seoul",
                 proactive_enabled=True, proactive_interval=1800, tts_voice=None):
        self.device = device
        self.model = None
        self.tokenizer = None
        self.tts_voice = tts_voice or "ko-KR-SunHiNeural"

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (í˜„ì¬ ì„¸ì…˜ìš©, LLM context window)
        self.conversation_history = []
        self.max_history = 20

        # êµ¬ì¡°í™”ëœ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.memory = MemoryManager(
            refresh_interval=300,    # 5ë¶„ë§ˆë‹¤ ìë™ refresh
            refresh_after_turns=5,   # 5í„´ë§ˆë‹¤ refresh
            idle_threshold=120       # 2ë¶„ idle ì‹œ refresh
        )

        # ì„œë¸Œì‹œìŠ¤í…œ
        self.emotion_system = EmotionSystem()
        self.info_services = InfoServices(weather_api_key, location)
        self.proactive = ProactiveInteraction(proactive_enabled, proactive_interval)
        self.scheduler = Scheduler()

    def load_model(self):
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            log.info(f"Loading Qwen2.5-0.5B-Instruct for Agent Mode on {self.device}...")
            model_name = "Qwen/Qwen2.5-0.5B-Instruct"

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name, torch_dtype=torch_dtype,
                device_map=self.device, trust_remote_code=True
            )
            log.info("Agent Mode LLM loaded.")

            # MemoryManagerì— LLM í•¨ìˆ˜ ì£¼ì…
            self.memory.set_llm(self._llm_generate)

        except ImportError:
            log.error("Transformers/Torch not installed. pip install transformers torch accelerate")
        except Exception as e:
            log.error(f"Failed to load Agent LLM: {e}")

    def _llm_generate(self, prompt: str, max_tokens=128) -> str:
        """MemoryManagerê°€ ì‚¬ìš©í•˜ëŠ” ë‚´ë¶€ LLM í˜¸ì¶œ"""
        if not self.model or not self.tokenizer:
            return ""
        try:
            messages = [
                {"role": "system", "content": "ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•´."},
                {"role": "user", "content": prompt}
            ]
            text_input = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.tokenizer([text_input], return_tensors="pt").to(self.device)
            generated = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.get("attention_mask"),
                max_new_tokens=max_tokens,
                do_sample=False, temperature=0.3
            )
            output_ids = generated[0][len(inputs.input_ids[0]):]
            return self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
        except Exception as e:
            log.error(f"LLM generate (internal) failed: {e}")
            return ""

    def generate_response(self, text: str, is_proactive: bool = False) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        if not self.model or not self.tokenizer:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            if not is_proactive:
                self.proactive.update_interaction()

                # ìˆ˜ë©´ ëª¨ë“œ ëª…ë ¹
                sleep_response = self._check_sleep_commands(text)
                if sleep_response:
                    return sleep_response

                # ì •ë³´ ìš”ì²­ (ë‚ ì”¨, ì‹œê°„ ë“±)
                info_response = self.info_services.process_info_request(text)
                if info_response:
                    return info_response

                # ì¼ì • ìš”ì²­
                schedule_response = self.scheduler.process_schedule_request(text)
                if schedule_response:
                    return schedule_response

            # ê°ì • ë¶„ì„
            self.emotion_system.analyze_emotion(text)

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ + ë©”ëª¨ë¦¬ì— ê¸°ë¡
            self.conversation_history.append({"role": "user", "content": text})
            self.memory.add_turn("user", text)

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
            messages = [{"role": "system", "content": self.memory.build_system_prompt()}]
            messages += [{"role": c["role"], "content": c["content"]}
                         for c in self.conversation_history[-self.max_history:]]

            text_input = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            model_inputs = self.tokenizer([text_input], return_tensors="pt").to(self.device)

            attention_mask = model_inputs.get("attention_mask")
            if attention_mask is None:
                pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id
                attention_mask = (model_inputs.input_ids != pad_id).long()

            generated_ids = self.model.generate(
                model_inputs.input_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                do_sample=True, temperature=0.8,
                top_p=0.9, repetition_penalty=1.1
            )

            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
            response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()

            # ì‘ë‹µ ê¸°ë¡
            self.conversation_history.append({"role": "assistant", "content": response})
            self.memory.add_turn("assistant", response)

            self.emotion_system.analyze_emotion(response)
            log.info(f"Agent Response: {response}")
            return response

        except Exception as e:
            log.error(f"LLM generation failed: {e}")
            return "ë¯¸ì•ˆ, ì ê¹ ì˜¤ë¥˜ê°€ ë‚¬ì–´."

    # â”€â”€ TTS â”€â”€

    async def _tts_gen(self, text, output_file):
        import edge_tts
        communicate = edge_tts.Communicate(text, self.tts_voice)
        await communicate.save(output_file)

    def text_to_audio(self, text: str) -> bytes:
        """TTS: í…ìŠ¤íŠ¸ë¥¼ 16kHz Mono PCM ì˜¤ë””ì˜¤ë¡œ ë³€í™˜"""
        try:
            import librosa

            tmp_mp3 = "temp_tts.mp3"
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            loop.run_until_complete(self._tts_gen(text, tmp_mp3))
            data, _ = librosa.load(tmp_mp3, sr=16000, mono=True)
            data = np.clip(data, -1.0, 1.0)
            return (data * 32767).astype(np.int16).tobytes()

        except ImportError:
            log.error("Install: pip install edge-tts librosa soundfile")
            return b""
        except Exception as e:
            log.error(f"TTS failed: {e}")
            return b""

    # â”€â”€ ê°ì •/ìˆ˜ë©´/í”„ë¡œì•¡í‹°ë¸Œ â”€â”€

    def get_emotion_command(self):
        return self.emotion_system.get_emotion_command()

    def _check_sleep_commands(self, text: str) -> Optional[str]:
        text_lower = text.lower()

        sleep_keywords = ["ì˜ê²Œ", "ì”ë‹¤", "ìëŸ¬", "ì˜ ì‹œê°„", "ìˆ˜ë©´", "ì¡°ìš©íˆ", "ê·¸ë§Œ ë§í•´"]
        if any(kw in text_lower for kw in sleep_keywords):
            return self.proactive.enter_sleep_mode()

        pause_keywords = ["ë©ˆì¶°", "ì¡°ìš©íˆ í•´", "ì‹œë„ëŸ¬", "ì ê¹ë§Œ", "ì¢€ ì‰¬ì–´"]
        if any(kw in text_lower for kw in pause_keywords):
            import re
            m = re.search(r'(\d+)\s*ì‹œê°„', text_lower)
            return self.proactive.pause_temporarily(int(m.group(1)) if m else 1)

        wake_keywords = ["ì¼ì–´ë‚˜", "ë‹¤ì‹œ ë§í•´", "ê¹¨ì›Œ", "ì‹œì‘"]
        if any(kw in text_lower for kw in wake_keywords):
            return self.proactive.wake_up()

        return None

    def get_proactive_message(self) -> Optional[str]:
        return self.proactive.get_proactive_message(
            current_emotion=self.emotion_system.current_emotion,
            important_memories=[]  # ë©”ëª¨ë¦¬ëŠ” ì´ì œ .md íŒŒì¼ì—ì„œ ê´€ë¦¬
        )

    def check_timers_and_alarms(self):
        messages = []
        for timer in self.info_services.check_timers():
            messages.append(f"â° {timer['label']} íƒ€ì´ë¨¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        for alarm in self.info_services.check_alarms():
            messages.append(f"â° {alarm['label']} ì•ŒëŒì…ë‹ˆë‹¤!")
        for schedule in self.scheduler.check_reminders():
            dt = datetime.fromisoformat(schedule["datetime"])
            messages.append(f"ğŸ“… {dt.strftime('%H:%M')}ì— '{schedule['title']}' ì¼ì •ì´ ìˆìŠµë‹ˆë‹¤!")
        return messages

    def clear_context(self):
        """ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ flush í›„)"""
        self.memory.refresh()
        self.conversation_history = []
        log.info("Context cleared. Memory persisted to .md files.")
