import logging
import io
import asyncio
import numpy as np
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Optional
from emotion_system import EmotionSystem
from info_services import InfoServices
from proactive_interaction import ProactiveInteraction
from scheduler import Scheduler

log = logging.getLogger("agent_mode")

class AgentMode:
    def __init__(self, device="cuda", weather_api_key=None, location="Seoul", 
                 proactive_enabled=True, proactive_interval=1800, tts_voice=None):
        self.device = device
        self.model = None
        self.tokenizer = None
        self.tts_voice = tts_voice or "ko-KR-SunHiNeural"
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
        self.conversation_history = []
        self.important_memories = []
        self.max_history = 20  # ìµœê·¼ 20ê°œ ëŒ€í™”ë§Œ ìœ ì§€
        self.context_backup_interval = 10  # 10ê°œ ëŒ€í™”ë§ˆë‹¤ ë°±ì—…
        self.conversation_count = 0
        
        # ê°ì • ì‹œìŠ¤í…œ
        self.emotion_system = EmotionSystem()
        
        # ì •ë³´ ì„œë¹„ìŠ¤
        self.info_services = InfoServices(weather_api_key, location)
        
        # í”„ë¡œì•¡í‹°ë¸Œ ìƒí˜¸ì‘ìš©
        self.proactive = ProactiveInteraction(proactive_enabled, proactive_interval)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = Scheduler()
        
        # ë°±ì—… ë””ë ‰í† ë¦¬
        self.backup_dir = Path("context_backup")
        self.backup_dir.mkdir(exist_ok=True)
        
        # ì‹œì‘ ì‹œ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ë³µì›
        self._restore_context()

    def load_model(self):
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            log.info(f"Loading Qwen2.5-0.5B-Instruct for Agent Mode on {self.device}...")
            model_name = "Qwen/Qwen2.5-0.5B-Instruct"
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch_dtype,
                device_map=self.device,
                trust_remote_code=True
            )
            log.info("Agent Mode LLM loaded.")
            
        except ImportError:
            log.error("Transformers/Torch not installed. pip install transformers torch accelerate")
        except Exception as e:
            log.error(f"Failed to load Agent LLM: {e}")

    def _get_personality_traits(self, personality: str) -> str:
        """ì„±ê²© íƒ€ì…ë³„ íŠ¹ì„± ë°˜í™˜"""
        traits = {
            "cheerful": "ë°ê³  í™œë°œí•˜ë©° ê¸ì •ì ì…ë‹ˆë‹¤. ëŒ€í™”ì—ì„œ ì¦ê±°ì›€ê³¼ ì—ë„ˆì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.",
            "calm": "ì°¨ë¶„í•˜ê³  ì•ˆì •ì ì´ë©° ì‹ ì¤‘í•©ë‹ˆë‹¤. í¸ì•ˆí•˜ê³  ë¯¿ì„ ìˆ˜ ìˆëŠ” ë¶„ìœ„ê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.",
            "playful": "ì¥ë‚œê¸° ìˆê³  ìœ ì¾Œí•˜ë©° ì°½ì˜ì ì…ë‹ˆë‹¤. ì¬ë¯¸ìˆëŠ” í‘œí˜„ì„ ìì£¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            "serious": "ì§„ì§€í•˜ê³  ì „ë¬¸ì ì´ë©° íš¨ìœ¨ì ì…ë‹ˆë‹¤. ì •í™•í•œ ì •ë³´ì™€ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤."
        }
        return traits.get(personality, traits["cheerful"])
    
    def _get_system_prompt(self) -> str:
        """í™ˆ ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        from config_loader import get_config
        config = get_config()
        assistant_config = config.get_assistant_config()
        
        assistant_name = assistant_config.get("name", "ì•„ì´")
        personality = assistant_config.get("personality", "cheerful")
        personality_trait = self._get_personality_traits(personality)
        
        memories_text = ""
        if self.important_memories:
            memories_text = "\n\nì¤‘ìš”í•œ ê¸°ì–µ:\n" + "\n".join(f"- {mem}" for mem in self.important_memories[-10:])
        
        return f"""ë‹¹ì‹ ì€ ê°€ì •ìš© AI í™ˆ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ë¦„ì€ '{assistant_name}'ì…ë‹ˆë‹¤.

ì„±ê²©: {personality_trait}

í•µì‹¬ ì—­í• :
1. ê°€ì¡± êµ¬ì„±ì›ë“¤ê³¼ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ëŒ€í™”
2. ì¼ìƒì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë„ì›€ ì œê³µ
3. ê°„ë‹¨í•œ ì •ë³´ ê²€ìƒ‰ ë° ì•ˆë‚´
4. ê°€ì¡±ì˜ ì¼ì •, ì„ í˜¸ì‚¬í•­, ì¤‘ìš”í•œ ì •ë³´ ê¸°ì–µ
5. ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì‘ë‹µ

ì¤‘ìš” ì›ì¹™:
- ëŒ€í™” ë‚´ìš©ì„ ì ˆëŒ€ ìŠì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤
- ì‚¬ìš©ìê°€ ì´ì „ì— ë§í•œ ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì°¸ì¡°í•˜ì„¸ìš”
- ê°€ì¡± êµ¬ì„±ì› ê°ìì˜ íŠ¹ì„±ê³¼ ì„ í˜¸ë¥¼ ê¸°ì–µí•˜ì„¸ìš”
- ì¤‘ìš”í•œ ë‚ ì§œ, ì•½ì†, ì„ í˜¸ì‚¬í•­ì€ ë°˜ë“œì‹œ ê¸°ì–µí•˜ì„¸ìš”
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´ì–´ê°€ì„¸ìš”

ì‘ë‹µ ìŠ¤íƒ€ì¼:
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”
- 2-3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
- ì„±ê²©ì— ë§ëŠ” ì–´ì¡° ìœ ì§€
- í•„ìš”ì‹œ ì´ì „ ëŒ€í™” ë‚´ìš© ì–¸ê¸‰
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê¸°
- ìì‹ ì„ '{assistant_name}'ì´ë¼ê³  ì†Œê°œí•˜ì„¸ìš”

í˜„ì¬ ê¸°ëŠ¥:
- ìŒì„± ëŒ€í™” (STT/TTS)
- ì„œë³´ ëª¨í„° ì œì–´ (ë¡œë´‡ ëª¨ë“œ ì „í™˜ ì‹œ)
- ì •ë³´ ì œê³µ ë° ëŒ€í™”
{memories_text}"""

    def generate_response(self, text: str, is_proactive: bool = False) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        if not self.model or not self.tokenizer:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—…ë°ì´íŠ¸ (í”„ë¡œì•¡í‹°ë¸Œê°€ ì•„ë‹ ë•Œë§Œ)
            if not is_proactive:
                self.proactive.update_interaction()
            
            # ìˆ˜ë©´ ëª¨ë“œ ëª…ë ¹ í™•ì¸
            if not is_proactive:
                sleep_response = self._check_sleep_commands(text)
                if sleep_response:
                    return sleep_response
            
            # ì •ë³´ ìš”ì²­ í™•ì¸ (ë‚ ì”¨, ì‹œê°„ ë“±)
            if not is_proactive:
                info_response = self.info_services.process_info_request(text)
                if info_response:
                    log.info(f"Info request processed: {text[:30]}...")
                    return info_response
                
                # ì¼ì • ìš”ì²­ í™•ì¸
                schedule_response = self.scheduler.process_schedule_request(text)
                if schedule_response:
                    log.info(f"Schedule request processed: {text[:30]}...")
                    return schedule_response
            
            # ê°ì • ë¶„ì„
            detected_emotion = self.emotion_system.analyze_emotion(text)
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.conversation_history.append({
                "role": "user",
                "content": text,
                "timestamp": datetime.now().isoformat(),
                "emotion": detected_emotion
            })
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            messages = [{"role": "system", "content": self._get_system_prompt()}]
            
            # ìµœê·¼ ëŒ€í™” ì¶”ê°€
            for conv in self.conversation_history[-self.max_history:]:
                messages.append({
                    "role": conv["role"],
                    "content": conv["content"]
                })
            
            text_input = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = self.tokenizer([text_input], return_tensors="pt").to(self.device)
            
            # Create attention mask explicitly to avoid warning
            attention_mask = model_inputs.get("attention_mask")
            if attention_mask is None:
                # If pad_token_id is same as eos_token_id, create attention mask manually
                attention_mask = (model_inputs.input_ids != self.tokenizer.pad_token_id).long()
                if self.tokenizer.pad_token_id is None:
                    # If no pad_token, use eos_token_id
                    attention_mask = (model_inputs.input_ids != self.tokenizer.eos_token_id).long()

            generated_ids = self.model.generate(
                model_inputs.input_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                repetition_penalty=1.1
            )
            
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]

            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            
            # ì‘ë‹µ ê°ì • ë¶„ì„
            response_emotion = self.emotion_system.analyze_emotion(response)
            
            # ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.conversation_history.append({
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now().isoformat(),
                "emotion": response_emotion
            })
            
            self.conversation_count += 1
            
            # ì£¼ê¸°ì  ë°±ì—…
            if self.conversation_count % self.context_backup_interval == 0:
                self._backup_context()
            
            # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
            self._extract_important_info(text, response)
            
            log.info(f"Agent Response: {response}")
            return response
            
        except Exception as e:
            log.error(f"LLM generation failed: {e}")
            return "ì£„ì†¡í•´ìš”, ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."

    def _extract_important_info(self, user_text: str, assistant_response: str):
        """ëŒ€í™”ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš” ì •ë³´ ê°ì§€
        important_keywords = [
            "ì´ë¦„", "ìƒì¼", "ì¢‹ì•„", "ì‹«ì–´", "ì•Œë ˆë¥´ê¸°", "ì•½ì†", "ì¼ì •",
            "ê°€ì¡±", "ì¹œêµ¬", "ì „í™”ë²ˆí˜¸", "ì£¼ì†Œ", "ê¸°ì–µ", "ìŠì§€ë§ˆ"
        ]
        
        combined_text = user_text + " " + assistant_response
        
        for keyword in important_keywords:
            if keyword in combined_text:
                memory_entry = f"[{datetime.now().strftime('%Y-%m-%d')}] {user_text[:50]}"
                if memory_entry not in self.important_memories:
                    self.important_memories.append(memory_entry)
                    log.info(f"Important memory saved: {memory_entry}")
                break
        
        # ìµœëŒ€ 50ê°œ ê¸°ì–µë§Œ ìœ ì§€
        if len(self.important_memories) > 50:
            self.important_memories = self.important_memories[-50:]

    def _backup_context(self):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë°±ì—…"""
        try:
            backup_data = {
                "timestamp": datetime.now().isoformat(),
                "conversation_count": self.conversation_count,
                "conversation_history": self.conversation_history[-self.max_history:],
                "important_memories": self.important_memories
            }
            
            backup_file = self.backup_dir / "latest_context.json"
            with open(backup_file, "w", encoding="utf-8") as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            # ë‚ ì§œë³„ ë°±ì—…ë„ ìƒì„±
            dated_backup = self.backup_dir / f"context_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(dated_backup, "w", encoding="utf-8") as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            log.info(f"Context backed up: {self.conversation_count} conversations")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (ìµœê·¼ 30ê°œë§Œ ìœ ì§€)
            self._cleanup_old_backups()
            
        except Exception as e:
            log.error(f"Context backup failed: {e}")

    def _restore_context(self):
        """ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë³µì›"""
        try:
            backup_file = self.backup_dir / "latest_context.json"
            if backup_file.exists():
                with open(backup_file, "r", encoding="utf-8") as f:
                    backup_data = json.load(f)
                
                self.conversation_history = backup_data.get("conversation_history", [])
                self.important_memories = backup_data.get("important_memories", [])
                self.conversation_count = backup_data.get("conversation_count", 0)
                
                log.info(f"Context restored: {len(self.conversation_history)} conversations, "
                        f"{len(self.important_memories)} memories")
            else:
                log.info("No previous context found, starting fresh")
                
        except Exception as e:
            log.error(f"Context restoration failed: {e}")

    def _cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        try:
            backup_files = sorted(self.backup_dir.glob("context_*.json"))
            if len(backup_files) > 30:
                for old_file in backup_files[:-30]:
                    old_file.unlink()
                    log.info(f"Deleted old backup: {old_file.name}")
        except Exception as e:
            log.error(f"Backup cleanup failed: {e}")

    async def _tts_gen(self, text, output_file):
        import edge_tts
        communicate = edge_tts.Communicate(text, self.tts_voice)
        await communicate.save(output_file)

    def text_to_audio(self, text: str) -> bytes:
        """TTS: í…ìŠ¤íŠ¸ë¥¼ 16kHz Mono PCM ì˜¤ë””ì˜¤ë¡œ ë³€í™˜"""
        try:
            import soundfile as sf
            import librosa
            
            tmp_mp3 = "temp_tts.mp3"
            
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
            loop.run_until_complete(self._tts_gen(text, tmp_mp3))
            
            data, samplerate = librosa.load(tmp_mp3, sr=16000, mono=True)
            data = np.clip(data, -1.0, 1.0)
            pcm_16 = (data * 32767).astype(np.int16)
            
            return pcm_16.tobytes()
            
        except ImportError:
            log.error("Install: pip install edge-tts librosa soundfile")
            return b""
        except Exception as e:
            log.error(f"TTS failed: {e}")
            return b""

    def get_emotion_command(self):
        """í˜„ì¬ ê°ì • ìƒíƒœì— ëŒ€í•œ ëª…ë ¹ ë°˜í™˜"""
        return self.emotion_system.get_emotion_command()
    
    def _check_sleep_commands(self, text: str) -> Optional[str]:
        """ìˆ˜ë©´/ë©ˆì¶¤ ëª…ë ¹ í™•ì¸"""
        text_lower = text.lower()
        
        # ìˆ˜ë©´ ëª¨ë“œ (ë‹¤ìŒë‚ ê¹Œì§€)
        sleep_keywords = ["ì˜ê²Œ", "ì”ë‹¤", "ìëŸ¬", "ì˜ ì‹œê°„", "ìˆ˜ë©´", "ì¡°ìš©íˆ", "ê·¸ë§Œ ë§í•´"]
        if any(keyword in text_lower for keyword in sleep_keywords):
            return self.proactive.enter_sleep_mode()
        
        # ì¼ì‹œ ì •ì§€
        pause_keywords = ["ë©ˆì¶°", "ì¡°ìš©íˆ í•´", "ì‹œë„ëŸ¬", "ì ê¹ë§Œ", "ì¢€ ì‰¬ì–´"]
        if any(keyword in text_lower for keyword in pause_keywords):
            # ì‹œê°„ ì¶”ì¶œ ì‹œë„
            import re
            hours_match = re.search(r'(\d+)\s*ì‹œê°„', text_lower)
            if hours_match:
                hours = int(hours_match.group(1))
                return self.proactive.pause_temporarily(hours)
            else:
                return self.proactive.pause_temporarily(1)  # ê¸°ë³¸ 1ì‹œê°„
        
        # ê¹¨ìš°ê¸°
        wake_keywords = ["ì¼ì–´ë‚˜", "ë‹¤ì‹œ ë§í•´", "ê¹¨ì›Œ", "ì‹œì‘"]
        if any(keyword in text_lower for keyword in wake_keywords):
            return self.proactive.wake_up()
        
        return None
    
    def get_proactive_message(self) -> Optional[str]:
        """í”„ë¡œì•¡í‹°ë¸Œ ë©”ì‹œì§€ ìƒì„±"""
        return self.proactive.get_proactive_message(
            current_emotion=self.emotion_system.current_emotion,
            important_memories=self.important_memories
        )
    
    def check_timers_and_alarms(self):
        """íƒ€ì´ë¨¸, ì•ŒëŒ, ì¼ì • ë¦¬ë§ˆì¸ë” í™•ì¸"""
        messages = []
        
        # íƒ€ì´ë¨¸ í™•ì¸
        expired_timers = self.info_services.check_timers()
        for timer in expired_timers:
            messages.append(f"â° {timer['label']} íƒ€ì´ë¨¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì•ŒëŒ í™•ì¸
        triggered_alarms = self.info_services.check_alarms()
        for alarm in triggered_alarms:
            messages.append(f"â° {alarm['label']} ì•ŒëŒì…ë‹ˆë‹¤!")
        
        # ì¼ì • ë¦¬ë§ˆì¸ë” í™•ì¸
        reminders = self.scheduler.check_reminders()
        for schedule in reminders:
            dt = datetime.fromisoformat(schedule["datetime"])
            time_str = dt.strftime("%H:%M")
            messages.append(f"ğŸ“… {time_str}ì— '{schedule['title']}' ì¼ì •ì´ ìˆìŠµë‹ˆë‹¤!")
        
        return messages
    
    def clear_context(self):
        """ì»¨í…ìŠ¤íŠ¸ ìˆ˜ë™ ì´ˆê¸°í™” (ë°±ì—… í›„)"""
        self._backup_context()
        self.conversation_history = []
        self.conversation_count = 0
        log.info("Context cleared (memories preserved)")
